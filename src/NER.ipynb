{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Αναγνώριση ονοματισμένων ονοτήτων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "\n",
    "\n",
    "def data_preparation_for_NER(data_filename, test_size=0.25):\n",
    "    \"\"\"\n",
    "    Given the file that contains annotated data for NER\n",
    "    splits the dataset to train set and test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_filename : str\n",
    "        the path to the file that contains the annotated\n",
    "        data for NER\n",
    "        \n",
    "    test_size : float\n",
    "        the test_size that is given as a parameter to\n",
    "        train_test_split function of scikit-learn\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        the tuple of train set and test set\n",
    "    \"\"\"\n",
    "    df_texts = pd.read_csv(data_filename, index_col=None, converters={'entities': eval})\n",
    "    df_texts['text_entities'] = list(zip(df_texts.text, df_texts.entities))\n",
    "    data = [(text, {'entities': entities}) for (text, entities) in df_texts['text_entities']]\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=10)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "def train_ner(labels, train_set, model=None, new_model_name=\"\", output_dir=None, n_iter=30):\n",
    "    \"\"\"\n",
    "    Sets up the pipeline and named entity recognizer\n",
    "    and trains the NER for new entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list\n",
    "        the labels (strings) of new entities \n",
    "        \n",
    "    train_set : list\n",
    "        the observations of the train set\n",
    "        \n",
    "    model : module\n",
    "        existing SpaCy model\n",
    "        \n",
    "    new_model_name : str\n",
    "        the name of the new model\n",
    "        \n",
    "    output_dir: str\n",
    "        the directory of the output file where the new\n",
    "        model will be saved\n",
    "        \n",
    "    n_iter : int\n",
    "        the number of the iterations in the training process\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = model.load()  # load existing spaCy model\n",
    "    else:\n",
    "        nlp = spacy.blank(\"el\")  # create blank model in greek\n",
    "    # add entity recognizer to model if it's not in the pipeline\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    for label in labels:\n",
    "        ner.add_label(label)  # add new entity labels to entity recognizer\n",
    "    \n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_set)\n",
    "            batches = minibatch(train_set, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses in iteration: \" + str(itn + 1), losses)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        \n",
    "        \n",
    "\n",
    "def evaluate_ner(output_dir, test_set, print_entities=False):\n",
    "    \"\"\"\n",
    "    Tests the new ner model and scores its performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir: str\n",
    "        the directory of the output file where the new\n",
    "        model was saved\n",
    "        \n",
    "    test_set : list\n",
    "        the observations of the test set\n",
    "        \n",
    "    print_entities : boolean\n",
    "        if True the entities detected for each text\n",
    "        in the test set will be printed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        all the metrics ant their scores that measure\n",
    "        the performance of the new NLP model\n",
    "    \"\"\"\n",
    "    ner_model = spacy.load(output_dir)\n",
    "    scorer = Scorer()\n",
    "    # test ner\n",
    "    for (input_, dict_entities) in test_set:        \n",
    "        [annot] = dict_entities.values()\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot)\n",
    "        pred_value = ner_model(input_)\n",
    "        # precision, recall, f-score for ner etc.\n",
    "        scorer.score(pred_value, gold)\n",
    "        if print_entities:\n",
    "            print(\"Entities in '%s'\" % input_[:70])\n",
    "            for ent in pred_value.ents:\n",
    "                print(ent.label_, ent.text)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_preparation_for_NER('../data/votana_total_texts_annotated.csv', 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Losses in iteration: 1 {'ner': 28009.486630714855}\n",
      "Losses in iteration: 2 {'ner': 10072.37744715274}\n",
      "Losses in iteration: 3 {'ner': 6750.224350848334}\n",
      "Losses in iteration: 4 {'ner': 5211.193739124763}\n",
      "Losses in iteration: 5 {'ner': 3992.39955358218}\n",
      "Losses in iteration: 6 {'ner': 3095.672196773336}\n",
      "Losses in iteration: 7 {'ner': 2830.3548845383857}\n",
      "Losses in iteration: 8 {'ner': 2343.4972213200845}\n",
      "Losses in iteration: 9 {'ner': 2095.902628214111}\n",
      "Losses in iteration: 10 {'ner': 1905.747405956969}\n",
      "Losses in iteration: 11 {'ner': 1849.0109994482434}\n",
      "Losses in iteration: 12 {'ner': 1549.1252833628105}\n",
      "Losses in iteration: 13 {'ner': 1275.264053523643}\n",
      "Losses in iteration: 14 {'ner': 1290.2932319212568}\n",
      "Losses in iteration: 15 {'ner': 1109.5565862497112}\n",
      "Losses in iteration: 16 {'ner': 1107.455720012933}\n",
      "Losses in iteration: 17 {'ner': 1111.0178881850686}\n",
      "Losses in iteration: 18 {'ner': 936.43680941124}\n",
      "Losses in iteration: 19 {'ner': 953.3981488042859}\n",
      "Losses in iteration: 20 {'ner': 907.7846608129182}\n",
      "Losses in iteration: 21 {'ner': 942.3204766935366}\n",
      "Losses in iteration: 22 {'ner': 859.58068365591}\n",
      "Losses in iteration: 23 {'ner': 674.2658956353648}\n",
      "Losses in iteration: 24 {'ner': 808.2670291873065}\n",
      "Losses in iteration: 25 {'ner': 747.9836518970394}\n",
      "Losses in iteration: 26 {'ner': 685.9839555140218}\n",
      "Losses in iteration: 27 {'ner': 700.8254331284348}\n",
      "Losses in iteration: 28 {'ner': 527.9219659444894}\n",
      "Losses in iteration: 29 {'ner': 619.454605340555}\n",
      "Losses in iteration: 30 {'ner': 626.3421904835702}\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "train_ner(['ORGAN', 'PLANT'], train_data, new_model_name='ethnopharmacology_ner', output_dir='./ner_model/', n_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "{'uas': 0.0, 'las': 0.0, 'las_per_type': {'': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'ents_p': 98.15357188335668, 'ents_r': 95.30168150346192, 'ents_f': 96.70660560818017, 'ents_per_type': {'PLANT': {'p': 97.71313267193543, 'r': 94.13575247043578, 'f': 95.8910891089109}, 'ORGAN': {'p': 99.52780692549842, 'r': 99.06005221932115, 'f': 99.29337869667627}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}\n"
     ]
    }
   ],
   "source": [
    "print('Testing...')\n",
    "scores = evaluate_ner('./ner_model/', test_data, False)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
