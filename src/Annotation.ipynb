{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Χαρακτηρισμός δεδομένων με αυτόματο τρόπο"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στόχος είναι να χαρακτηρίσουμε με \"έξυπνο\" τρόπο το σύνολο των κειμένων με τις οντότητες που περιέχει το καθένα (και τις θέσεις αυτών στο κείμενο) ώστε να μπορέσουν να χρησιμοποιηθούν σε επόμενο στάδιο για την εκπαίδευση του Named Entity Recognizer. Αυτή η επισήμανση μπορεί να γίνει με αυτόματο τρόπο, πραγματοποιώντας αναζήτηση στα κείμενα με βάση γνωστές οντότητες που μας ενδιαφερούν στην παρούσα εργασία (βότανα, όργανα ανθρώπινου σώματος κλπ.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φτιάχνουμε 4 αρχεία txt, σε κάθενα από τα οποία βρίσκεται αποθηκευμένη μία λίστα με όρους που θα χρησιμοποιηθούν στην αναζήτηση. Στα αρχεία organs και plants βρίσκονται λέξεις που αποτελούν οντότητες των αντίστοιχων κατηγοριών ενώ στα αρχεία multiword_organs και multiword_plants βρίσκονται πάλι γνωστές οντότητες αυτών των κατηγοριών που αποτελούνται από παραπάνω από μία λέξεις."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Για να ταιριάξουμε τους όρους αναζήτησης στις λίστες που διαβάσαμε από τα txt, θα χρειαστεί διασπάσουμε τα κείμενα σε λέξεις (tokens). Αυτό γίνεται με τον tokenizer της spaCy για τα Ελληνικά. Επειδή όμως οι λέξεις μπορεί να βρίσκονται σε διάφορες πτώσεις πρέπει να κάνουμε lemmatization ή stemming τόσο στους όρους αναζήτησης όσο και στις λέξεις των κειμένων, ώστε να είναι επιτυχημένο το string matching που θα ακολουθήσει. \n",
    "\n",
    "Δοκιμάσαμε το lemmatizer της spaCy τόσο από το small όσο και από το medium model που διατίθενται για την ελληνική γλώσσα. Αν και το medium απέδωσε καλύτερα, γενικά η λημματοποίηση δεν είχε όσο ικανοποιητικά αποτελέσματα θα θέλαμε. Στα αποτελέσματα παρέμεναν διαφορετικές πτώσεις και πολλές φορές λανθασμένες παραποιήσεις της αρχικής λέξης, όπως θα δούμε στη συνέχεια. Για αυτό στραφήκαμε στη βιβλιοθήκη greek_stemmer που κάνει αποκατάληξη στα Ελληνικά, όπως λέει και το όνομά της. Συγκρίναμε τις δύο μεθόδους σε διάφορα παραδείγματα και όπως φαίνεται παρακάτω ο greek-stemmer αποδίδει καλύτερα και τα αποτελέσματά του φαίνονται να οδηγούν σε καλύτερο string matching αργότερα.\n",
    "\n",
    "Και στις δύο μεθόδους παρατηρήθηκε ότι οι τόνοι μπορεί να οδηγούσαν σε διαφορετικά αποτελέσματα για την ίδια ρίζα, οπότε πρέπει να αφαιρεθούν προτού προχωρήσουμε με τη διαδικασία. Επίσης, η εναλλαγή πεζών κεφαλαίων κάποιες φορές οδήγησε σε διαφορετικά αποτελέσματα, οπότε επιλέχθηκε να μετατραπούν όλες οι λέξεις σε μία από τις δύο μορφές. Μάλιστα, ο greek-stemmer φαίνεται να λειτουργεί μόνο με κεφαλαία οπότε επιλέχθηκε αυτή η μετατροπή.\n",
    "\n",
    "Οι μικρές διαφορές στις πτώσεις και στις καταλήξεις θα μπορούσαν να αντιμετωπιστούν εφαρμόζοντας fuzzy string matching, με υψηλό κατώφλι χαλάρωσης. Ωστόσο υπάρχουν περιπτώσεις όπου απαιτούνταν χαμηλότερο κατώφλι για να ταιριάξουν δυο λέξεις με κοινή ρίζα ενώ ένα πολύ ψηλό επαρκούσε για να ταιριάξουν δύο τελειώς διαφορετικές λέξεις, όπως φαίνεται και στο παρακάτω παράδειγμα. Τέτοια προβλήματα εξακολούθησαν να υπάρχουν, πιο σπάνια βέβαια, ακόμα και όταν συνδυάστηκε η λημματοποίηση του spaCy με fuzzy string matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω φαίνονται κάποια παραδείγματα που μας οδήγησαν σε αυτές τις αποφάσεις."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "print(fuzz.ratio('έντερα', 'εντέρου'))\n",
    "print(fuzz.ratio('καρδιά', 'καρυδιά'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import el_core_news_md\n",
    "nlp = el_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from greek_stemmer import GreekStemmer\n",
    "stemmer = GreekStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω παραθέτουμε κάποια παραδείγματα όπου φαίνεται η σύγκριση της λημματοποίησης με spaCy και της αποκατάληξης με greek_stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΚΑΡΔΙΑ καρδιας ΚΑΡΔ\n",
      "ΚΑΡΔΙΑ καρδιας ΚΑΡΔ\n",
      "ΚΑΡΔΙΑ καρδιας ΚΑΡΔ\n",
      "ΚΑΡΔΙΑΣ καρδια ΚΑΡΔ\n",
      "ΚΑΡΔΙΑΣ ΚΑΡΔΙΑΣ ΚΑΡΔ\n",
      "ΚΑΡΔΙΑΣ ΚΑΡΔΙΑΣ ΚΑΡΔ\n",
      "ΚΑΡΔΙΕΣ ΚΑΡΔΙΕΣ ΚΑΡΔ\n",
      "ΚΑΡΔΙΕΣ ΚΑΡΔΙΕΣ ΚΑΡΔ\n",
      "ΚΑΡΔΙΩΝ ΚΑΡΔΙΩΝ ΚΑΡΔ\n",
      "ΚΑΡΔΙΩΝ καρδιων ΚΑΡΔ\n",
      "ΚΑΡΔΙΩΝ καρδιων ΚΑΡΔ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('καρδια ΚΑΡΔΙΑ Καρδια καρδιας Καρδιας ΚΑΡΔΙΑΣ καρδιες ΚΑΡΔΙΕΣ καρδιων Καρδιων ΚΑΡΔΙΩΝ'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΕΝΤΕΡΑ εντερα ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΑ εντερα ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΩΝ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΑ εντερα ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΩΝ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΩΝ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΩΝ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΟ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΟ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΟ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΟΥ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΟΥ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΟΥ εντερο ΕΝΤΕΡ\n",
      "ΕΝΤΕΡΟΥ εντερο ΕΝΤΕΡ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('εντερα Εντερα εντερων ΕΝΤΕΡΑ Εντερων εντερων ΕΝΤΕΡΩΝ εντερο Εντερο ΕΝΤΕΡΟ εντερου Εντερου ΕΝΤΕΡΟΥ εντερου'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΟΥΡΟΠΟΙΗΤΙΚΟ ουροποιητικος ΟΥΡΟΠΟΙΗΤ\n",
      "ΟΥΡΟΠΟΙΗΤΙΚΟΥ ουροποιητικο ΟΥΡΟΠΟΙΗΤ\n",
      "ΟΥΡΟΠΟΙΗΤΙΚΟΥ ουροποιητικο ΟΥΡΟΠΟΙΗΤ\n",
      "ΟΥΡΟΠΟΙΗΤΙΚΟΥ ουροποιητικο ΟΥΡΟΠΟΙΗΤ\n",
      "ΟΥΡΟΠΟΙΗΤΙΚΟΥ ουροποιητικο ΟΥΡΟΠΟΙΗΤ\n",
      "ΟΥΡΟΠΟΙΗΤΙΚΟ ουροποιητικος ΟΥΡΟΠΟΙΗΤ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('ουροποιητικο ουροποιητικου ουροποιητικου ουροποιητικου ΟΥΡΟΠΟΙΗΤΙΚΟΥ Ουροποιητικο'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΕΝΔΟΚΡΙΝΕΙΣ ενδοκρινεις ΕΝΔΟΚΡΙΝ\n",
      "ΕΝΔΟΚΡΙΝΕΙΣ ενδοκρινεις ΕΝΔΟΚΡΙΝ\n",
      "ΕΝΔΟΚΡΙΝΩΝ ενδοκρινος ΕΝΔΟΚΡΙΝ\n",
      "ΕΝΔΟΚΡΙΝΩΝ ενδοκρινος ΕΝΔΟΚΡΙΝ\n",
      "ΕΝΔΟΚΡΙΝΩΝ ενδοκρινος ΕΝΔΟΚΡΙΝ\n",
      "ΕΝΔΟΚΡΙΝΗΣ ΕΝΔΟΚΡΙΝΗΣ ΕΝΔΟΚΡΙΝ\n",
      "ΕΝΔΟΚΡΙΝΗΣ ΕΝΔΟΚΡΙΝΗΣ ΕΝΔΟΚΡΙΝ\n",
      "ΕΝΔΟΚΡΙΝΗΣ ΕΝΔΟΚΡΙΝΗΣ ΕΝΔΟΚΡΙΝ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('ενδοκρινεις Ενδοκρινεις Ενδοκρινων ΕΝΔΟΚΡΙΝΩΝ ενδοκρινων ενδοκρινης Ενδοκρινης ΕΝΔΟΚΡΙΝΗΣ'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΩΝ ωοθηκων ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΗ ωοθηκη ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΩΝ ωοθηκων ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΩΝ ωοθηκων ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΗ ωοθηκη ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n",
      "ΩΟΘΗΚΕΣ ωοθηκα ΩΟΘΗΚ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('ωοθηκες Ωοθηκες ΩΟΘΗΚΕΣ ωοθηκων ωοθηκη Ωοθηκων ΩΟΘΗΚΩΝ Ωοθηκη ωοθηκες ωοθηκες ωοθηκες ωοθηκες ωοθηκες'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΡΙΝΙΚΟΣ ρινικος ΡΙΝ\n",
      "ΡΙΝΙΚΗ ρινικη ΡΙΝ\n",
      "ΡΙΝΙΚΗ ρινικη ΡΙΝ\n",
      "ΡΙΝΙΚΗ ρινικη ΡΙΝ\n",
      "ΡΙΝΙΚΕΣ ΡΙΝΙΚΕΣ ΡΙΝ\n",
      "ΡΙΝΙΚΕΣ ΡΙΝΙΚΕΣ ΡΙΝ\n",
      "ΡΙΝΙΚΕΣ ΡΙΝΙΚΕΣ ΡΙΝ\n",
      "ΡΙΝΙΚΕΣ ΡΙΝΙΚΕΣ ΡΙΝ\n",
      "ΡΙΝΙΚΕΣ ΡΙΝΙΚΕΣ ΡΙΝ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('ρινικος ρινικη Ρινικη ΡΙΝΙΚΗ ρινικες ΡΙΝΙΚΕΣ ρινικες ρινικες ρινικες'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΣΜΗΓΜΑΤΟΓΟΝΟΣ ΣΜΗΓΜΑΤΟΓΟΝΟΣ ΣΜΗΓΜΑΤΟΓΟΝ\n",
      "ΣΜΗΓΜΑΤΟΓΟΝΟΙ ΣΜΗΓΜΑΤΟΓΟΝΟΙ ΣΜΗΓΜΑΤΟΓΟΝ\n",
      "ΣΜΗΓΜΑΤΑΓΟΝΟΙ σμηγματαγονοι ΣΜΗΓΜΑΤΑΓΟΝ\n",
      "ΣΜΗΓΜΑΤΟΓΟΝΩΝ ΣΜΗΓΜΑΤΟΓΟΝΩΝ ΣΜΗΓΜΑΤΟΓΟΝ\n",
      "ΣΜΗΓΜΑΤΟΓΟΝΩΝ ΣΜΗΓΜΑΤΟΓΟΝΩΝ ΣΜΗΓΜΑΤΟΓΟΝ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('σμηγματογονος σμηγματογονοι Σμηγματαγονοι σμηγματογονων ΣΜΗΓΜΑΤΟΓΟΝΩΝ'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΑΓΚΙΝΑΡΑ αγκιναρας ΑΓΚΙΝΑΡ\n",
      "ΑΓΚΙΝΑΡΕΣ αγκιναρε ΑΓΚΙΝΑΡ\n",
      "ΑΓΚΙΝΑΡΕΣ αγκιναρε ΑΓΚΙΝΑΡ\n",
      "ΑΓΚΙΝΑΡΩΝ αγκιναρων ΑΓΚΙΝΑΡ\n",
      "ΑΓΚΙΝΑΡΑΣ αγκιναρας ΑΓΚΙΝΑΡ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('αγκιναρα αγκιναρες ΑΓΚΙΝΑΡΕΣ αγκιναρων Αγκιναρας'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΑΝΗΘΟΣ ανηθος ΑΝΗΘ\n",
      "ΑΝΗΘΟΥ ανηθο ΑΝΗΘ\n",
      "ΑΝΗΘΟ ανηθο ΑΝΗΘ\n",
      "ΑΝΗΘΟ ανηθο ΑΝΗΘ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('Ανηθος ανηθου ανηθο ΑΝΗΘΟ'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΔΑΜΑΣΚΗΝΙΑ δαμασκηνια ΔΑΜΑΣΚΗΝ\n",
      "ΔΑΜΑΣΚΗΝΑ δαμασκηνα ΔΑΜΑΣΚΗΝ\n",
      "ΔΑΜΑΣΚΗΝΟ δαμασκηνο ΔΑΜΑΣΚΗΝ\n",
      "ΔΑΜΑΣΚΗΝΩΝ δαμασκηνο ΔΑΜΑΣΚΗΝ\n",
      "ΔΑΜΑΣΚΗΝΟΥ δαμασκηνου ΔΑΜΑΣΚΗΝ\n",
      "ΔΜΑΣΚΗΝΙΑΣ δμασκηνιας ΔΜΑΣΚΗΝ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('Δαμασκηνια δαμασκηνα ΔΑΜΑΣΚΗΝΟ δαμασκηνων Δαμασκηνου δμασκηνιας'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΚΡΕΜΜΥΔΙ ΚΡΕΜΜΥΔΙ ΚΡΕΜΜΥΔ\n",
      "ΚΡΕΜΜΥΔΑ κρεμμυδο ΚΡΕΜΜΥΔ\n",
      "ΚΡΕΜΜΥΔΙΑ κρεμμυδια ΚΡΕΜΜΥΔ\n",
      "ΚΡΕΜΜΥΔΙΩΝ κρεμμυδιων ΚΡΕΜΜΥΔ\n",
      "ΚΡΕΜΜΥΔΙΟΥ κρεμμυδιου ΚΡΕΜΜΥΔ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('κρεμμυδι Κρεμμυδα κρεμμυδια κρεμμυδιων Κρεμμυδιου'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΚΥΣΤΗ κυστη ΚΥΣΤ\n",
      "ΚΥΣΤΗΣ κυστη ΚΥΣΤ\n",
      "ΚΥΣΤΕΩΣ κυστεως ΚΥΣΤ\n",
      "ΚΥΣΤΩΝ ΚΥΣΤΩΝ ΚΥΣΤ\n",
      "ΚΥΣΤΕΣ κυστα ΚΥΣΤ\n",
      "ΚΥΣΤΕΙΣ κυστει ΚΥΣΤ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('κυστη κυστης κυστεως κυστων κυστες κυστεις'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΑΔΕΝΑΣ αδενας ΑΔΕΝ\n",
      "ΑΔΕΝΩΝ αδενο ΑΔΕΝ\n",
      "ΑΔΕΝΑ αδενας ΑΔΕΝ\n"
     ]
    }
   ],
   "source": [
    "t = nlp('αδενας αδενων αδενα'.upper())\n",
    "for token in t:\n",
    "    print(token.text,token.lemma_,stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import el_core_news_md\n",
    "nlp = el_core_news_md.load()\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "stop_words |= {'.',',',';','?',':','!',' ','&','/','ή','-','(',')','[',']','{','}','\"','\"','`','~','\\xa0'}\n",
    "\n",
    "from greek_stemmer import GreekStemmer\n",
    "stemmer = GreekStemmer()\n",
    "\n",
    "def search_terms_from_txt(filename, encoding):\n",
    "    \"\"\"\n",
    "    Reads the content of a txt file and produces a list of strings \n",
    "    -in our case a list of search terms- separated by comma.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        the path (relative or absolute) of the txt file \n",
    "        which contains search terms\n",
    "    encoding : str\n",
    "        the encoding of the txt file which contains search terms\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of strings - search terms (mutliword or singleword)\n",
    "    \"\"\"\n",
    "    with open(filename,'r',encoding=encoding) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    for term in content:\n",
    "        search_terms = term.split(',')\n",
    "    return search_terms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stemming(word):\n",
    "    \"\"\"\n",
    "    Implements stemming and preparation before it for a given word.\n",
    "    Preparation involves removing accents and capitalizing letters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        the word that is about to be stemmed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        the stem of the given word at uppercase letters\n",
    "    \"\"\"\n",
    "    return stemmer.stem(word.replace('ά', 'α').replace('έ', 'ε').replace('ή', 'η').replace('ί', 'ι').replace('ό', 'ο')\n",
    "                        .replace('ύ', 'υ').replace('ώ', 'ω').replace('ϊ', 'ι').replace('ϋ', 'υ').replace('ΐ', 'ι')\n",
    "                        .replace('ΰ', 'υ').replace('Ά', 'Α').replace('Έ', 'Ε').replace('Ή', 'Η').replace('Ί', 'Ι')\n",
    "                        .replace('Ό', 'Ο').replace('Ύ', 'Υ').replace('Ϊ', 'Ι').replace('Ϋ', 'Υ').upper())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multiword_terms_look_up_list(search_terms_list):\n",
    "    \"\"\"\n",
    "    Given a list of search terms where each of them contains\n",
    "    multiple words, produces a list of lists with the stems\n",
    "    of their tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_terms_list : list\n",
    "        the list that contains the multiword search terms\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of lists, each of them containing the stems of \n",
    "        the tokens of one multiword search term\n",
    "    \"\"\"\n",
    "    look_up_list_parts = []\n",
    "    for term in search_terms_list:\n",
    "        stem_parts = [stemming(part) for part in term.split(' ')]\n",
    "        look_up_list_parts +=[stem_parts]\n",
    "    return look_up_list_parts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def singleword_terms_look_up_list(search_terms_list):\n",
    "    \"\"\"\n",
    "    Given a list of singleword search terms, produces a list of \n",
    "    lists with their stems.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_terms_list : list\n",
    "        the list that contains the singleword search terms\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of the stems of the elements of the given list\n",
    "    \"\"\"\n",
    "    return list(np.unique([stemming(term) for term in search_terms_list]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text_to_tokens_stems(text):\n",
    "    \"\"\"\n",
    "    Given a text, produces a list of its stemmed tokens, excluding\n",
    "    the stop words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        the given text that is about to be tokenized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a list of tuples that contain the tokens of the given text,\n",
    "        the stemmed text of them (str), the start position (int) \n",
    "        and the end position (int) of them in te text and a boolean \n",
    "        variable which indicates whether the token has been already\n",
    "        identified as a part of an entity or not\n",
    "    \"\"\"\n",
    "    text2 = nlp(text)\n",
    "    tokens_list = []\n",
    "    # keep token, text of token, start and end index of token in text \n",
    "    # and whether it belongs to an entity or not\n",
    "    for token in text2:\n",
    "        tokens_list += [(token, token.text, token.idx, token.idx + len(token), False)]\n",
    "    # remove stop words\n",
    "    tokens = [(a,b,c,d,e) for (a,b,c,d,e) in tokens_list if not(a.is_stop or (a.lower_ in stop_words) \n",
    "                                                                or (a.lemma_ in stop_words))]\n",
    "    # preparation and stemming of each token\n",
    "    final_tokens = [(a, stemming(b), c, d,e) for (a,b,c,d,e) in tokens]\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def annotation(input_csv_filename, known_entities_with_labels, output_csv_filename):\n",
    "    \"\"\"\n",
    "    Annotates the texts in an input csv file with their entities.\n",
    "    The entities are detected through a search process using \n",
    "    predefined already known entities and their labels. The \n",
    "    annotated data are stored in a new output csv file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_csv_file : str\n",
    "        the path (relative or absolute) of the file that contains\n",
    "        the texts\n",
    "        \n",
    "    known_entities_with_labels : list\n",
    "        list of tuples each of them referring to a category of entities\n",
    "        each tuple contains a path of a txt file with singleword\n",
    "        search terms - known entities (str), a path of a txt file with\n",
    "        multiword search terms - known entities (str) and the label of\n",
    "        the category of entities that they belong (str)\n",
    "        \n",
    "    output_csv_filename : str\n",
    "        the path (relative or absolute) of the output file where the\n",
    "        annotated data will be stored        \n",
    "    \"\"\"\n",
    "    df_texts = pd.read_csv(input_csv_filename)\n",
    "    df_texts['entities'] = np.empty((len(df_texts), 0)).tolist()\n",
    "    \n",
    "    entities_lists = [(multiword_terms_look_up_list(search_terms_from_txt(multiword_list_filename, 'utf-8')), \n",
    "                       singleword_terms_look_up_list(search_terms_from_txt(singleword_list_filename, 'utf-8')), label) \n",
    "                      for (singleword_list_filename, multiword_list_filename, label) in known_entities_with_labels]\n",
    "            \n",
    "    # for each text\n",
    "    for k in range(df_texts.shape[0]):\n",
    "        entities = []\n",
    "        text = df_texts.iloc[k]['text']\n",
    "        #tokenization and stemming of the text\n",
    "        text_tokens = text_to_tokens_stems(text)\n",
    "        \n",
    "        # firstly annotation of multiword entities \n",
    "        for (multiword_look_up_list, _, label) in entities_lists:\n",
    "            for i in range(len(text_tokens)):\n",
    "                (token, token_string, start_pos, end_pos, annotated) = text_tokens[i]\n",
    "                for terms_list in multiword_look_up_list:\n",
    "                    match = True\n",
    "                    for j in range(len(terms_list)):\n",
    "                        # string matching for each token in a multiword entity-search term\n",
    "                        (_,b,_,next_end_pos,_) = text_tokens[i+j]\n",
    "                        if b != terms_list[j]:\n",
    "                            match = False\n",
    "                            break\n",
    "                    if match:\n",
    "                        for l in range(len(terms_list)):\n",
    "                            token_list = list(text_tokens[i+l])\n",
    "                            #set annotated=True for each token in the entity \n",
    "                            # to avoid double annotation as a singleword entity as well\n",
    "                            token_list[4] = True     \n",
    "                            text_tokens[i+l] = tuple(token_list)\n",
    "                        entities += [(start_pos, next_end_pos, label)]\n",
    "        \n",
    "        # annotation of singleword entities \n",
    "        for (_, singleword_look_up_list, label) in entities_lists:\n",
    "            for (token, token_string, start_pos, end_pos, annotated) in text_tokens:\n",
    "                for term in singleword_look_up_list:\n",
    "                    if  token_string == term and not annotated:\n",
    "                        entities += [(start_pos, end_pos, label)]\n",
    "\n",
    "        df_texts.at[k,'entities'] = df_texts.iloc[k]['entities'] + entities\n",
    "        \n",
    "    df_texts.to_csv(output_csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε πρώτη φάση θα ασχοληθούμε μόνο με τα κείμενα από τη σελίδα \"Προϊόντα της Φύσης\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation('../data/proionta_tis_fisis_votana.csv',[('../data/organs.txt', '../data/multiword_organs.txt', 'ORGAN'),\n",
    "                                                    ('../data/plants.txt', '../data/multiword_plants.txt', 'PLANT')], \n",
    "          '../data/proionta_tis_fisis_votana_annotated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Είναι σημαντικό πως πρέπει να κρατήσουμε τη θέση , βασικά την αρχή και το τέλος κάθε token. Αυτό είναι απαραίτητο, καθώς στα χαρακτηρισμένα δεδομένα πέρα από τις οντότητες που περιέχονται σε ένα κείμενο πρέπει να υπάχρουν και οι θέσεις εμφάνισης αυτών.\n",
    "\n",
    "Επίσης, για να επιταχυνθεί η διαδικασία και να αποφευχθεί το ταίριασμα κάποιου όρου αναζήτησης με μία stop word, οι τελευταίες απορρίπτονται πριν αρχίσει η διαδικασία της αναζήτησης. Με αυτόν τον τρόπο γλιτώνουμε κάποιες άσκοπες συγκρίσεις.\n",
    "\n",
    "Επίσης, φροντίζουμε να εντοπίσουμε πρώτα τις οντότητες που αποτελούνται από παραπάνω λέξεις και μετά θα επισημάνουμε τις υπόλοιπες εφόσον δεν ανήκουν σε κάποια από τις προηγούμενες. Αυτό γίνεται γιατί ως οντότητες υπάρχουν π.χ. τόσο η \"χοληδόχος κύστη\" όσο και η \"χοληδόχος\" σκέτο. Αν αναγνωριζόταν πρώτα η μονολεξική οντότητα, η άλλη δε θα εντοπιζόταν ποτέ.\n",
    "\n",
    "Τέλος, για να λάβουμε καλύτερα αποτελέσματα δε λάβαμε υπόψη μας κάποιες οντότητες. Πιο συγκεκριμένα, αφαιρέθηκαν οι αμυγδαλές ως όργανο του λεμφικού συστήματος για να μην υπάρχει conflict με τις αμυγδαλιές και τα αμύγδαλα ως οντότητες των βοτάνων, τα οποία εμφανίζονταν πολύ συχνότερα. Επίσης, αφαιρέσαμε τη λέυκα και το βήχιο από τα βότανα, καθώς υπήρχε μπέρδεμα με το λευκό χρώμα και το βήχα, τα οποία επίσης εμφανίζονται πολύ πιο συχνά. Ακόμα, αφαιρέθηκε ο θύμος αδένας από το σύστημα ενδοκρινών και το λεμφικό σύστημα, καθώς στα κείμενο ο θύμος συναντάται μόνο ως αρχαία ονομασία του θυμαριού. Ακόμα, οι σύνδεσμοι μόνο μία φορά εμφανίζονταν με την επιθυμητή σημασία που σχετίζεται με το ανθρώπινο σώμα και όλες τις άλλες με άλλη καθημερινή σημασία. Τέλος, από τα βότανα αφαιρέθηκε και η φούσκα, γιατί υπάρχει και στο ανθρώπινο σώμα."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
