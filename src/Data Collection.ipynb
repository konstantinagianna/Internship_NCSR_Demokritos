{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Συλλογή δεδομένων\n",
    "\n",
    "Επικεντρωθήκαμε στην κοινότητα \"Φυτά, Βότανα Μυστικά\" του Facebook όπου οι διαχειριστές κοινοποιούν άρθρα δημοσιευμένα σε σελίδες και blogs. Η συντριπτική πλειψηφία των αναρτήσεων προέρχεται από τη σελίδα [\"Προϊόντα της Φύσης\"](https://www.proionta-tis-fisis.com/). Επίσης, υπάρχουν λίγες αναρτήσεις από τις εξής πηγές [medialabnews.gr IATRIKANEA](https://medlabgr.blogspot.com/), [itrofi](https://www.itrofi.gr/), οι οποίες έχουν ειδικές κατηγορίες για βότανα και διατροφή αλλά και τις [Εναλλακτική Δράση](https://enallaktikidrasi.com/), [awekengr.com](https://www.awakengr.com/), [EarthShareMe.com](https://earthshareme.com/), [flowmag](https://www.flowmagazine.gr/), στις οποίες αναρτώνται καποια σχετικά άρθρα ανάμεσα σε άλλα. Σκεφτήκαμε να συλλέξουμε τα δεδομένα κατευθείαν από τις σελίδες/blogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re        \n",
    "\n",
    "\n",
    "\n",
    "def get_embedded_links_in_a_webpage(webpage, url_prefix):\n",
    "    \"\"\"\n",
    "    Gets the links that are embedded in a webpage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    webpage : str\n",
    "        the url of the webpage that we want to scrap\n",
    "        \n",
    "    url_prefix : str\n",
    "        the beginning of the links that is not included\n",
    "        in the 'href' attribute if any\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        the list of the embedded links in the given webpage\n",
    "    \"\"\"\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(webpage)\n",
    "    #discard irrelevant links\n",
    "    return [url_prefix + link['href'] for link in BeautifulSoup(response, parse_only=SoupStrainer('a')).find_all('a', href=True)]\n",
    "\n",
    "\n",
    "\n",
    "def get_embedded_links_in_multiple_pages(homepage, start_page_num, end_page_num, slash_at_the_end, url_prefix):\n",
    "    \"\"\"\n",
    "    Iterates over multiple pages in a website and retrieves\n",
    "    the embedded links in each of them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    homepage : str\n",
    "        the standard invariable part of the link for every page\n",
    "        \n",
    "    start_page_num : int\n",
    "        the number of the first page\n",
    "        \n",
    "    end_page_num : int\n",
    "        the number of the last page\n",
    "        \n",
    "    slash_at_the_end : boolean\n",
    "        True if there is a slash at the end of the page url\n",
    "        else False\n",
    "    \n",
    "    url_prefix : str\n",
    "        the beginning of the retrieved links that is not \n",
    "        included in the 'href' attribute if any\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        the list of the embedded links in the desired pages\n",
    "        of a given website\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    for counter in range(start_page_num,end_page_num):\n",
    "        webpage = homepage + str(counter) + ('/' if slash_at_the_end else '')\n",
    "        links += get_embedded_links_in_a_webpage(webpage, url_prefix)\n",
    "    links = np.unique(links)\n",
    "    return links\n",
    "\n",
    "\n",
    "\n",
    "def remove_irrelevant_links(links):\n",
    "    \"\"\"\n",
    "    Removes irrelevant embedded links like ads, menu options etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    links : list\n",
    "        the initial list of links that probably contains\n",
    "        irrelevant links\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        the cleaned list that is consisted only of useful links\n",
    "    \"\"\"\n",
    "    return [link for link in links if (('https://www.proionta-tis-fisis.com' in link and ('category' not in link) and \n",
    "                                        ('#' not in link) and ('epikoinonia' not in link) and ('shop' not in link) and\n",
    "                                        ('twitter' not in link) and ('facebook' not in link) and ('author' not in link) and \n",
    "                                        ('javascript' not in link) and ('mailto' not in link) and \n",
    "                                        ('oroi-xrisis-istotopou' not in link) and \n",
    "                                        (link != 'https://www.proionta-tis-fisis.com/prosoxi') and \n",
    "                                        (link != 'https://www.proionta-tis-fisis.com/') and\n",
    "                                        (link != 'https://www.proionta-tis-fisis.com') and\n",
    "                                        (link != 'https://www.proionta-tis-fisis.com/i-zoi-einai-mikri-gia-na-zoume-kleismenoi-sto-asfales-koutaki-mas/') and\n",
    "                                        (link != 'https://www.proionta-tis-fisis.com/synaisthimatiki-yperfagia-aisthima-katoterotitas-kai-antimetopisi/') and\n",
    "                                        (link != 'https://www.proionta-tis-fisis.com/to-synoliko-oikologiko-apotypoma-ton-ananeosimon-pigon-energeias-den-ehei-dierevnithei-eparkos/') and\n",
    "                                        (link != 'https://www.proionta-tis-fisis.com/tropoi-syntirisis-ton-trofimon-pro-psygeiou/') and\n",
    "                                        (link != 'https://www.proionta-tis-fisis.com/yparhei-zoi-prin-ton-thanato/'))\n",
    "                                       or ('https://www.itrofi.gr/fytika/votana/article/' in link))]\n",
    "\n",
    "\n",
    "\n",
    "def get_article_in_url(url):\n",
    "    \"\"\"\n",
    "    Retrieves text in the page of the given url.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        the url of the webpage that is about to be scrapped\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        the text displayed in the given url\n",
    "    \"\"\"\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "\n",
    "\n",
    "\n",
    "def get_articles(links):\n",
    "    \"\"\"\n",
    "    Retrieves all the articles from the given urls.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    links : list\n",
    "        the list of the links of the webpages that will be\n",
    "        scrapped\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        the list of the articles-texts located in the given urls\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    for link in links:\n",
    "        article = get_article_in_url(link)\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "\n",
    "\n",
    "def clean_articles(articles, start_phrase, end_phrase):\n",
    "    \"\"\"\n",
    "    Removes irrelevant content in the beginning and at the end\n",
    "    of an article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    articles : list\n",
    "        the list of the given articles\n",
    "        \n",
    "    start_phrase : str\n",
    "        the first breaking point that the text will be splitted\n",
    "        \n",
    "    end_phrase : str\n",
    "        the second breaking point that the text will be splitted\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        the list of the cleaned articles after removing the unnecessary\n",
    "        content in the beginning and at the end of each one of them\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for article in articles:\n",
    "        article = article.split(start_phrase, 1)[1]\n",
    "        article = article.split(end_phrase, 1)[0]\n",
    "        cleaned.append(article.strip())\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(homepage, start_page_num, end_page_num, slash_at_the_end, url_prefix, output_csv_filename, start_phrase, end_phrase):\n",
    "    \"\"\"\n",
    "    Creates the final dataset where each observation consists of\n",
    "    the link and the relevant text-article.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    homepage : str\n",
    "        the standard invariable part of the link for every page\n",
    "        \n",
    "    start_page_num : int\n",
    "        the number of the first page\n",
    "        \n",
    "    end_page_num : int\n",
    "        the number of the last page\n",
    "        \n",
    "    slash_at_the_end : boolean\n",
    "        True if there is a slash at the end of the page url\n",
    "        else False\n",
    "        \n",
    "    url_prefix : str\n",
    "        the beginning of the retrieved links that is not \n",
    "        included in the 'href' attribute if any\n",
    "        \n",
    "    output_csv_filename : str\n",
    "        the path of the output csv file where the dataset\n",
    "        will be stored\n",
    "        \n",
    "    start_phrase : str\n",
    "        the first breaking point that the text will be splitted\n",
    "        \n",
    "    end_phrase : str\n",
    "        the second breaking point that the text will be splitted\n",
    "    \"\"\"\n",
    "    links = get_embedded_links_in_multiple_pages(homepage, start_page_num, end_page_num, slash_at_the_end, url_prefix)\n",
    "    links = remove_irrelevant_links(links)\n",
    "    articles = get_articles(links)\n",
    "    articles = clean_articles(articles, start_phrase, end_phrase)    \n",
    "    df = pd.DataFrame(list(zip(links, articles)), columns =['link', 'text']) \n",
    "    df.to_csv(output_csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_dataset('https://www.itrofi.gr/fytika/votana?page=', 0, 10, False, 'https://www.itrofi.gr', '../data/itrofi_votana.csv',\n",
    "              'ΒΟΤΑΝΑ          ', 'Tags:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset('https://www.proionta-tis-fisis.com/category/votana/page/', 1, 68, True, '', '../data/proionta_tis_fisis_votana.csv',\n",
    "              'ΠερισσοτεραΑναζήτησηΑρχική', 'Προηγούμενο άρθρο')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
